{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4094648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8c0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from cvClassifier import logger\n",
    "from cvClassifier.utils.common import get_size, read_yaml, create_directories, save_json\n",
    "from cvClassifier.constants import *\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, \\\n",
    "                                       MulticlassF1Score, MulticlassAUROC\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fb795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data_path: Path\n",
    "    validation_data_path: Path\n",
    "    trained_model_path: Path\n",
    "    best_params_path: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: int\n",
    "    # Hyperparameter search space\n",
    "    learning_rate_range: list  \n",
    "    batch_size_options: list   \n",
    "    epochs_options: list     \n",
    "    n_trials: int             # Number of trials for optimization\n",
    "    timeout: int              # Timeout in seconds\n",
    "\n",
    "    mlflow_uri: str\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa0ffca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    def get_model_training_config(self) -> ModelTrainingConfig:\\n        ' Gets the config details for the model training pipeline '\\n        config = self.config.model_training\\n        params = self.params\\n        \\n        create_directories([config.root_dir])\\n\\n        model_training_config = ModelTrainingConfig(\\n            root_dir = config.root_dir,\\n            updated_base_model_path = config.updated_base_model_path,\\n            training_data_path = config.training_data,\\n            validation_data_path = config.validation_data,\\n            trained_model_path = config.trained_model_path,\\n            params_epochs = params.EPOCHS,\\n            params_batch_size = params.BATCH_SIZE,\\n            params_is_augmentation = params.AUGMENTATION,\\n            params_image_size = params.IMAGE_SIZE,\\n            params_learning_rate = self.params.LEARNING_RATE\\n        )\\n\\n        return model_training_config\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConfigurationManager:\n",
    "    # this class manages the configuration of the model preparation pipeline\n",
    "\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "            ' Gets the config details for the hyperparameter tuning pipeline '\n",
    "            config = self.config.model_training\n",
    "            params = self.params\n",
    "            training_data = config.training_data\n",
    "            validation_data = config.validation_data\n",
    "\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            model_training_config = ModelTrainingConfig(\n",
    "                root_dir = config.root_dir,\n",
    "                updated_base_model_path = config.updated_base_model_path,\n",
    "                training_data_path = Path(training_data),\n",
    "                validation_data_path = Path(validation_data),\n",
    "                trained_model_path = config.trained_model_path,\n",
    "                best_params_path = Path(config.root_dir) / \"best_params.json\",\n",
    "                params_epochs = params.EPOCHS,\n",
    "                params_batch_size = params.BATCH_SIZE,\n",
    "                params_is_augmentation = params.AUGMENTATION,\n",
    "                params_image_size = params.IMAGE_SIZE,\n",
    "                # Hyperparameter search space\n",
    "                learning_rate_range = params.LEARNING_RATE_RANGE,  # [min_lr, max_lr]\n",
    "                batch_size_options = params.BATCH_SIZE_OPTIONS ,   # Different batch sizes to try\n",
    "                epochs_options = params.EPOCHS_OPTIONS,        # Different epoch counts to try\n",
    "                n_trials = params.N_TRIALS,                       # Number of trials for optimization\n",
    "                timeout = params.TIMEOUT,                        # Timeout in seconds (1 hour)\n",
    "                mlflow_uri = config.mlflow_tracking_uri\n",
    "            )\n",
    "\n",
    "            return model_training_config\n",
    "\n",
    "'''\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        ' Gets the config details for the model training pipeline '\n",
    "        config = self.config.model_training\n",
    "        params = self.params\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            updated_base_model_path = config.updated_base_model_path,\n",
    "            training_data_path = config.training_data,\n",
    "            validation_data_path = config.validation_data,\n",
    "            trained_model_path = config.trained_model_path,\n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_image_size = params.IMAGE_SIZE,\n",
    "            params_learning_rate = self.params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return model_training_config\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff503b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as requests\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b35c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # For multi-class metrics:\n",
    "        self.val_precision = MulticlassPrecision(num_classes=4, average='weighted', validate_args=True)\n",
    "        self.val_recall = MulticlassRecall(num_classes=4, average='weighted', validate_args=True)\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=4, average='weighted', validate_args=True)\n",
    "        self.val_auc = MulticlassAUROC(num_classes=4, average='weighted', validate_args=True)\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "        preds_probs = torch.softmax(outputs, dim=-1) # Probabilities for AUROC and other metrics if needed\n",
    "        preds_labels = outputs.argmax(dim=-1) # Predicted class labels for precision/recall/f1 if not using probabilities\n",
    "\n",
    "        # Update with probabilities for AUROC, and with predicted labels/probabilities for others\n",
    "        # Depending on the metric, it might expect one-hot encoded targets or class indices.\n",
    "        # For Multiclass metrics, typically raw logits or probabilities and integer class labels are used.\n",
    "        self.val_precision.update(preds_probs, labels)\n",
    "        self.val_recall.update(preds_probs, labels)\n",
    "        self.val_f1.update(preds_probs, labels)\n",
    "        self.val_auc.update(preds_probs, labels)\n",
    "\n",
    "        \n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate average metrics\n",
    "        if self.test_step_outputs:\n",
    "            avg_loss = torch.stack([x['test_loss'] for x in self.test_step_outputs]).mean()\n",
    "            avg_acc = torch.stack([x['test_acc'] for x in self.test_step_outputs]).mean()\n",
    "            \n",
    "            self.log('avg_test_loss', avg_loss)\n",
    "            self.log('avg_test_acc', avg_acc)\n",
    "            \n",
    "            # Clear the list for next epoch\n",
    "            self.test_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        avg_precision = self.val_precision.compute()\n",
    "        avg_recall = self.val_recall.compute()\n",
    "        avg_f1 = self.val_f1.compute()\n",
    "        avg_auc = self.val_auc.compute()\n",
    "\n",
    "        self.log('val_precision', avg_precision, prog_bar=True)\n",
    "        self.log('val_recall', avg_recall, prog_bar=True)\n",
    "        self.log('val_f1_score', avg_f1, prog_bar=True)\n",
    "        self.log('val_auc_roc', avg_auc, prog_bar=True)\n",
    "\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_f1.reset()\n",
    "        self.val_auc.reset()\n",
    "\n",
    "        # Calculate average metrics\n",
    "        if self.validation_step_outputs:\n",
    "            avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()\n",
    "            avg_acc = torch.stack([x['val_acc'] for x in self.validation_step_outputs]).mean()\n",
    "            \n",
    "            self.log('avg_val_loss', avg_loss)\n",
    "            self.log('avg_val_acc', avg_acc)\n",
    "            \n",
    "            # Clear the list for next epoch\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a67c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def train(self):\\n        # Create Lightning model\\n        lightning_model = LightningModel(\\n            model=self.model,\\n            learning_rate=self.config.params_learning_rate\\n        )\\n        \\n        # Create trainer with automatic logging and progress bars\\n        trainer = pl.Trainer(\\n            max_epochs=self.config.params_epochs,\\n            accelerator=\\'auto\\',  # Automatically use GPU if available\\n            devices=\\'auto\\',      # Use all available devices\\n            logger=True,         # Enable logging\\n            enable_progress_bar=True,\\n            enable_model_summary=True,\\n            enable_checkpointing=True,\\n            log_every_n_steps=50,\\n        )\\n        \\n        logger.info(\"Starting training with PyTorch Lightning...\")\\n        \\n        # Train the model (this replaces all your manual training loop!)\\n        trainer.fit(\\n            model=lightning_model,\\n            train_dataloaders=self.train_loader,\\n            val_dataloaders=self.valid_loader\\n        )\\n        \\n        # Get final metrics\\n        train_metrics = trainer.callback_metrics\\n        \\n        logger.info(\"Training completed!\")\\n        logger.info(\"=\" * 60)\\n        logger.info(\"FINAL TRAINING METRICS:\")\\n        \\n        # Print final metrics\\n        for key, value in train_metrics.items():\\n            if isinstance(value, torch.Tensor):\\n                logger.info(f\"{key}: {value.item():.4f}\")\\n            else:\\n                logger.info(f\"{key}: {value}\")\\n        \\n        logger.info(\"=\" * 60)\\n        \\n        # Save the trained model\\n        self.save_model(\\n            path=self.config.trained_model_path,\\n            model=lightning_model.model  # Extract the actual model\\n        )\\n        \\n        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\\n        \\n        # Return training history for analysis\\n        return trainer.callback_metrics\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Set up MLflow\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        mlflow.set_experiment(\"hyperparameter_tuning\")\n",
    "\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        logger.info(f\"Model loaded from {self.config.updated_base_model_path}\")\n",
    "\n",
    "    def train_valid_generator(self, batch_size = None):\n",
    "\n",
    "        # preparing the validation dataset\n",
    "        valid_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Converts to tensor and scales to [0,1] (equivalent to rescale=1./255)\n",
    "        ])\n",
    "        \n",
    "        # preparing the training dataset\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.config.params_image_size[:-1]),\n",
    "                transforms.RandomRotation(40),  # rotation_range=40\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # horizontal_flip=True\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.2, 0.2),  # width_shift_range=0.2, height_shift_range=0.2\n",
    "                    scale=(0.8, 1.2),  # zoom_range=0.2\n",
    "                    shear=0.2  # shear_range=0.2\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            train_transforms = valid_transforms\n",
    "\n",
    "\n",
    "        # load training dataset\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=self.config.training_data_path,\n",
    "            transform=train_transforms\n",
    "        )\n",
    "        logger.info(f\"Training dataset created from {self.config.training_data_path}\")\n",
    "\n",
    "        # load validation dataset\n",
    "        valid_dataset = datasets.ImageFolder(\n",
    "            root=self.config.validation_data_path,\n",
    "            transform=valid_transforms\n",
    "        )\n",
    "        logger.info(f\"Validation dataset created from {self.config.validation_data_path}\")\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.params_batch_size if batch_size == None else batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.config.params_batch_size if batch_size == None else batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.train_dataset_size = len(train_dataset)\n",
    "        self.valid_dataset_size = len(valid_dataset)\n",
    "        \n",
    "        logger.info(f\"Training samples: {self.train_dataset_size}\")\n",
    "        logger.info(f\"Validation samples: {self.valid_dataset_size}\")\n",
    "        logger.info(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "        logger.info(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "        return self.train_loader, self.valid_loader\n",
    "    \n",
    "    def get_hyperparameter_search_space(self):\n",
    "        \"\"\"Define hyperparameter search space\"\"\"\n",
    "        return {\n",
    "            'learning_rate': self.config.learning_rate_range,\n",
    "            'batch_size': self.config.batch_size_options,\n",
    "            'epochs': self.config.epochs_options\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    \n",
    "        search_space = self.get_hyperparameter_search_space()\n",
    "    \n",
    "        # Suggest hyperparameters using the search space\n",
    "        learning_rate = trial.suggest_float('learning_rate', \n",
    "                                        search_space['learning_rate'][0], \n",
    "                                        search_space['learning_rate'][-1], \n",
    "                                        log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', search_space['batch_size'])\n",
    "        epochs = trial.suggest_categorical('epochs', search_space['epochs'])\n",
    "        \n",
    "        logger.info(f\"Trial {trial.number}: lr={learning_rate:.6f}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "        with mlflow.start_run(nested=True) as run:\n",
    "            \n",
    "            # log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                'learning_rate': learning_rate,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs\n",
    "            })\n",
    "            # get base model\n",
    "            self.get_base_model()\n",
    "            # Create data loaders with suggested batch size\n",
    "            train_loader, valid_loader = self.train_valid_generator(batch_size)\n",
    "\n",
    "            # create lightning model instance\n",
    "            lightning_model = LightningModel(\n",
    "                model=self.model,\n",
    "                learning_rate=learning_rate\n",
    "            )\n",
    "\n",
    "            # create an mlflow logger\n",
    "            mlflow_logger = MLFlowLogger(\n",
    "                experiment_name=\"hyperparameter_tuning\",\n",
    "                tracking_uri=self.config.mlflow_uri,\n",
    "                run_id=run.info.run_id # Associated with the current MLflow run\n",
    "            )\n",
    "\n",
    "            # create new pytorch lightning trainer\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=epochs,\n",
    "                accelerator='auto',\n",
    "                devices='auto',\n",
    "                logger=mlflow_logger,\n",
    "                enable_progress_bar=False, # Disable progress bar for cleaner Optuna output\n",
    "                enable_model_summary=False,\n",
    "                log_every_n_steps=10\n",
    "            )\n",
    "\n",
    "            # fit the model\n",
    "            trainer.fit(\n",
    "                model=lightning_model,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=valid_loader\n",
    "            )\n",
    "\n",
    "            # Retrieve metrics from trainer.callback_metrics after training\n",
    "            val_loss = trainer.callback_metrics.get('val_loss')\n",
    "            val_acc = trainer.callback_metrics.get('val_acc')\n",
    "            train_loss = trainer.callback_metrics.get('train_loss')\n",
    "            train_acc = trainer.callback_metrics.get('train_acc')\n",
    "\n",
    "            # log metrics to mlflow\n",
    "            trial_metrics = {}\n",
    "            if val_loss is not None:\n",
    "                trial_metrics['val_loss'] = val_loss.item() if hasattr(val_loss, 'item') else val_loss\n",
    "            if val_acc is not None:\n",
    "                trial_metrics['val_acc'] = val_acc.item() if hasattr(val_acc, 'item') else val_acc\n",
    "            if train_loss is not None:\n",
    "                trial_metrics['train_loss'] = train_loss.item() if hasattr(train_loss, 'item') else train_loss\n",
    "            if train_acc is not None:\n",
    "                trial_metrics['train_acc'] = train_acc.item() if hasattr(train_acc, 'item') else train_acc\n",
    "\n",
    "            mlflow.log_metrics(trial_metrics)\n",
    "\n",
    "            # Return the validation loss for Optuna to minimize\n",
    "            if 'val_loss' in trial_metrics:\n",
    "                return trial_metrics['val_loss']\n",
    "            else:\n",
    "                logger.warning(f\"Validation loss not found for Trial {trial.number}. Returning inf.\")\n",
    "                return float('inf')\n",
    "    \n",
    "\n",
    "    def optimize_hyperparameters(self):\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=self.config.n_trials,\n",
    "            timeout=self.config.timeout\n",
    "        )\n",
    "        \n",
    "        # Get best parameters\n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        logger.info(f\"Best parameters: {self.best_params}\")\n",
    "        logger.info(f\"Best validation loss: {self.best_score:.4f}\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        best_params_path = Path(self.config.root_dir)/\"best_params.json\"\n",
    "        save_json(best_params_path, self.best_params)\n",
    "        \n",
    "        return self.best_params\n",
    "\n",
    "    def log_trial_to_mlflow(self, trial_params, trial_metrics, lightning_model):\n",
    "        \"\"\"Log trial results to MLflow\"\"\"\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_params(trial_params)\n",
    "            mlflow.log_metrics(trial_metrics)\n",
    "            \n",
    "            # Log model\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.pytorch.log_model(\n",
    "                    lightning_model.model, \n",
    "                    \"model\", \n",
    "                    registered_model_name=\"VGG16Model\"\n",
    "                )\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(lightning_model.model, \"model\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train with hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Run hyperparameter optimization first\n",
    "        self.optimize_hyperparameters()\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        return self.train_with_best_params()\n",
    "\n",
    "    def train_with_best_params(self):\n",
    "        \"\"\"Train the final model with best hyperparameters\"\"\"\n",
    "        if self.best_params is None:\n",
    "            logger.warning(\"No best parameters found. Running optimization first...\")\n",
    "            self.optimize_hyperparameters()\n",
    "        \n",
    "        # Create data loaders with best batch size\n",
    "        train_loader, valid_loader = self.train_valid_generator(self.best_params['batch_size'])\n",
    "        \n",
    "        # Create Lightning model with best learning rate\n",
    "        lightning_model = LightningModel(\n",
    "            model=self.model,\n",
    "            learning_rate=self.best_params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Set up model checkpointing\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=self.config.root_dir,\n",
    "            filename=\"best_model\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1\n",
    "        )\n",
    "\n",
    "        # Set up MLflow logger for final training\n",
    "        mlflow_logger = MLFlowLogger(\n",
    "            experiment_name=\"best_parameters_training\",\n",
    "            tracking_uri=self.config.mlflow_uri\n",
    "        )\n",
    "        \n",
    "        # Create trainer with best epochs\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.best_params['epochs'],\n",
    "            accelerator='auto',\n",
    "            devices='auto',\n",
    "            logger=mlflow_logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            log_every_n_steps=50,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.fit(\n",
    "            model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader\n",
    "        )\n",
    "        \n",
    "        # Save the final model\n",
    "        torch.save(lightning_model.model, self.config.trained_model_path)\n",
    "        \n",
    "        logger.info(f\"Final model saved to {self.config.trained_model_path}\")\n",
    "        \n",
    "        return trainer.callback_metrics\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    def train(self):\n",
    "        # Create Lightning model\n",
    "        lightning_model = LightningModel(\n",
    "            model=self.model,\n",
    "            learning_rate=self.config.params_learning_rate\n",
    "        )\n",
    "        \n",
    "        # Create trainer with automatic logging and progress bars\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.config.params_epochs,\n",
    "            accelerator='auto',  # Automatically use GPU if available\n",
    "            devices='auto',      # Use all available devices\n",
    "            logger=True,         # Enable logging\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            enable_checkpointing=True,\n",
    "            log_every_n_steps=50,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Starting training with PyTorch Lightning...\")\n",
    "        \n",
    "        # Train the model (this replaces all your manual training loop!)\n",
    "        trainer.fit(\n",
    "            model=lightning_model,\n",
    "            train_dataloaders=self.train_loader,\n",
    "            val_dataloaders=self.valid_loader\n",
    "        )\n",
    "        \n",
    "        # Get final metrics\n",
    "        train_metrics = trainer.callback_metrics\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"FINAL TRAINING METRICS:\")\n",
    "        \n",
    "        # Print final metrics\n",
    "        for key, value in train_metrics.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                logger.info(f\"{key}: {value.item():.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Save the trained model\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=lightning_model.model  # Extract the actual model\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\n",
    "        \n",
    "        # Return training history for analysis\n",
    "        return trainer.callback_metrics\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bcbb152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 16:47:54,134: INFO: common]: yaml file successfully loaded from config/config.yaml\n",
      "[2025-07-18 16:47:54,137: INFO: common]: yaml file successfully loaded from params.yaml\n",
      "[2025-07-18 16:47:54,138: INFO: common]: Directory created at: artifacts\n",
      "[2025-07-18 16:47:54,139: INFO: common]: Directory created at: artifacts/model_training\n",
      "[2025-07-18 16:47:54,378: INFO: 2171364587]: Using device: cpu\n",
      "[2025-07-18 16:47:54,411: INFO: 2171364587]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-18 16:47:54,414: INFO: 2171364587]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-18 16:47:54,415: INFO: 2171364587]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-18 16:47:54,416: INFO: 2171364587]: Training samples: 613\n",
      "[2025-07-18 16:47:54,416: INFO: 2171364587]: Validation samples: 315\n",
      "[2025-07-18 16:47:54,416: INFO: 2171364587]: Number of classes: 4\n",
      "[2025-07-18 16:47:54,417: INFO: 2171364587]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-18 16:47:54,417: INFO: 2171364587]: Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1xz8vq817d3_x9b35qzvvgjc0000gn/T/ipykernel_52386/2171364587.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
      "[I 2025-07-18 16:47:54,418] A new study created in memory with name: no-name-d1d227f5-71be-4369-9aeb-3829a890737a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 16:47:54,419: INFO: 2171364587]: Trial 0: lr=0.002369, batch_size=16, epochs=20\n",
      "[2025-07-18 16:47:54,866: INFO: 2171364587]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-18 16:47:54,869: INFO: 2171364587]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-18 16:47:54,870: INFO: 2171364587]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-18 16:47:54,871: INFO: 2171364587]: Training samples: 613\n",
      "[2025-07-18 16:47:54,871: INFO: 2171364587]: Validation samples: 315\n",
      "[2025-07-18 16:47:54,872: INFO: 2171364587]: Number of classes: 4\n",
      "[2025-07-18 16:47:54,872: INFO: 2171364587]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-18 16:47:54,906: INFO: setup]: GPU available: True (mps), used: True\n",
      "[2025-07-18 16:47:54,906: INFO: setup]: TPU available: False, using: 0 TPU cores\n",
      "[2025-07-18 16:47:54,907: INFO: setup]: HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1xz8vq817d3_x9b35qzvvgjc0000gn/T/ipykernel_52386/2171364587.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 16:59:29,014: INFO: fit_loop]: `Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 16:59:29,684] Trial 0 finished with value: 2.7212462425231934 and parameters: {'learning_rate': 0.0023688639503640775, 'batch_size': 16, 'epochs': 20}. Best is trial 0 with value: 2.7212462425231934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 16:59:29,686: INFO: 2171364587]: Trial 1: lr=0.005106, batch_size=32, epochs=50\n",
      "[2025-07-18 16:59:30,124: INFO: 2171364587]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-18 16:59:30,127: INFO: 2171364587]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-18 16:59:30,129: INFO: 2171364587]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-18 16:59:30,129: INFO: 2171364587]: Training samples: 613\n",
      "[2025-07-18 16:59:30,130: INFO: 2171364587]: Validation samples: 315\n",
      "[2025-07-18 16:59:30,130: INFO: 2171364587]: Number of classes: 4\n",
      "[2025-07-18 16:59:30,131: INFO: 2171364587]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-18 16:59:30,146: INFO: setup]: GPU available: True (mps), used: True\n",
      "[2025-07-18 16:59:30,147: INFO: setup]: TPU available: False, using: 0 TPU cores\n",
      "[2025-07-18 16:59:30,148: INFO: setup]: HPU available: False, using: 0 HPUs\n",
      "[2025-07-18 17:10:28,856: INFO: call]: \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-18 17:10:29,631] Trial 1 failed with parameters: {'learning_rate': 0.005105903209394756, 'batch_size': 32, 'epochs': 50} because of the following error: NameError(\"name 'exit' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 212, in advance\n",
      "    batch, _, __ = next(data_fetcher)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py\", line 133, in __next__\n",
      "    batch = super().__next__()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py\", line 60, in __next__\n",
      "    batch = next(self.iterator)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 78, in __next__\n",
      "    out[i] = next(self.iterators[i])\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n",
      "    return img.convert(\"RGB\")\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/PIL/Image.py\", line 995, in convert\n",
      "    self.load()\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/PIL/ImageFile.py\", line 293, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/d9/1xz8vq817d3_x9b35qzvvgjc0000gn/T/ipykernel_52386/2171364587.py\", line 148, in objective\n",
      "    trainer.fit(\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 64, in _call_and_handle_interrupt\n",
      "    exit(1)\n",
      "NameError: name 'exit' is not defined\n",
      "[W 2025-07-18 17:10:29,639] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    263\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[1;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain_valid_generator()\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[7], line 235\u001b[0m, in \u001b[0;36mModelTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train with hyperparameter optimization\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Run hyperparameter optimization first\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_with_best_params()\n",
      "Cell \u001b[0;32mIn[7], line 192\u001b[0m, in \u001b[0;36mModelTraining.optimize_hyperparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m    187\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    188\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Get best parameters\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[7], line 148\u001b[0m, in \u001b[0;36mModelTraining.objective\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    137\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    138\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    139\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Retrieve metrics from trainer.callback_metrics after training\u001b[39;00m\n\u001b[1;32m    155\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_model_training_config()\n",
    "    training = ModelTraining(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
