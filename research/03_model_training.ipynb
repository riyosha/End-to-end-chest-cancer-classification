{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4094648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8c0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvClassifier import logger\n",
    "from cvClassifier.utils.common import get_size, read_yaml, create_directories \n",
    "from cvClassifier.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6fb795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data_path: Path\n",
    "    validation_data_path: Path\n",
    "    trained_model_path: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: int\n",
    "    params_learning_rate: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    # this class manages the configuration of the model preparation pipeline\n",
    "\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        ''' Gets the config details for the model training pipeline '''\n",
    "        config = self.config.model_training\n",
    "        params = self.params\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            updated_base_model_path = config.updated_base_model_path,\n",
    "            training_data_path = config.training_data,\n",
    "            validation_data_path = config.validation_data,\n",
    "            trained_model_path = config.trained_model_path,\n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_image_size = params.IMAGE_SIZE,\n",
    "            params_learning_rate = self.params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ff503b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 17:26:42,066: INFO: font_manager]: Failed to extract font properties from /System/Library/Fonts/Supplemental/NISC18030.ttf: In FT2Font: Could not set the fontsize (invalid pixel size; error code 0x17)\n",
      "[2025-07-05 17:26:42,195: INFO: font_manager]: Failed to extract font properties from /System/Library/Fonts/LastResort.otf: tuple indices must be integers or slices, not str\n",
      "[2025-07-05 17:26:42,248: INFO: font_manager]: Failed to extract font properties from /System/Library/Fonts/Apple Color Emoji.ttc: In FT2Font: Could not set the fontsize (invalid pixel size; error code 0x17)\n",
      "[2025-07-05 17:26:42,257: INFO: font_manager]: generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as requests\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        # Store outputs for epoch-level metrics\n",
    "        self.test_step_outputs.append({'test_loss': loss, 'test_acc': acc})\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate average metrics\n",
    "        if self.test_step_outputs:\n",
    "            avg_loss = torch.stack([x['test_loss'] for x in self.test_step_outputs]).mean()\n",
    "            avg_acc = torch.stack([x['test_acc'] for x in self.test_step_outputs]).mean()\n",
    "            \n",
    "            self.log('avg_test_loss', avg_loss)\n",
    "            self.log('avg_test_acc', avg_acc)\n",
    "            \n",
    "            # Clear the list for next epoch\n",
    "            self.test_step_outputs.clear()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a67c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def train(self):\\n\\n        self.steps_per_epoch = len(self.train_loader)\\n        self.validation_steps = len(self.valid_loader)\\n        \\n        criterion = nn.CrossEntropyLoss()\\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.params_learning_rate)\\n        \\n        for epoch in range(self.config.params_epochs):\\n            # Training phase\\n            self.model.train()\\n            for inputs, labels in self.train_loader:\\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\\n                optimizer.zero_grad()\\n                loss = criterion(self.model(inputs), labels)\\n                loss.backward()\\n                optimizer.step()\\n            \\n            # Validation phase\\n            self.model.eval()\\n            with torch.no_grad():\\n                for inputs, labels in self.valid_loader:\\n                    inputs, labels = inputs.to(self.device), labels.to(self.device)\\n                    criterion(self.model(inputs), labels) \\n        \\n        logger.info(f\"Training completed for {self.config.params_epochs} epochs\")\\n        self.save_model(\\n            path=self.config.trained_model_path,\\n            model=self.model\\n        )\\n\\n        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\\n    '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        logger.info(f\"Model loaded from {self.config.updated_base_model_path}\")\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "\n",
    "        # preparing the validation dataset\n",
    "        valid_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Converts to tensor and scales to [0,1] (equivalent to rescale=1./255)\n",
    "        ])\n",
    "        \n",
    "        # preparing the training dataset\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.config.params_image_size[:-1]),\n",
    "                transforms.RandomRotation(40),  # rotation_range=40\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # horizontal_flip=True\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.2, 0.2),  # width_shift_range=0.2, height_shift_range=0.2\n",
    "                    scale=(0.8, 1.2),  # zoom_range=0.2\n",
    "                    shear=0.2  # shear_range=0.2\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            train_transforms = valid_transforms\n",
    "\n",
    "\n",
    "        # load training dataset\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=self.config.training_data_path,\n",
    "            transform=train_transforms\n",
    "        )\n",
    "        logger.info(f\"Training dataset created from {self.config.training_data_path}\")\n",
    "\n",
    "        # load validation dataset\n",
    "        valid_dataset = datasets.ImageFolder(\n",
    "            root=self.config.validation_data_path,\n",
    "            transform=valid_transforms\n",
    "        )\n",
    "        logger.info(f\"Validation dataset created from {self.config.validation_data_path}\")\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.train_dataset_size = len(train_dataset)\n",
    "        self.valid_dataset_size = len(valid_dataset)\n",
    "        \n",
    "        logger.info(f\"Training samples: {self.train_dataset_size}\")\n",
    "        logger.info(f\"Validation samples: {self.valid_dataset_size}\")\n",
    "        logger.info(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "        logger.info(f\"Classes: {train_dataset.classes}\")\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: nn.Module):\n",
    "        torch.save(model, path)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # Create Lightning model\n",
    "        lightning_model = LightningModel(\n",
    "            model=self.model,\n",
    "            learning_rate=self.config.params_learning_rate\n",
    "        )\n",
    "        \n",
    "        # Create trainer with automatic logging and progress bars\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.config.params_epochs,\n",
    "            accelerator='auto',  # Automatically use GPU if available\n",
    "            devices='auto',      # Use all available devices\n",
    "            logger=True,         # Enable logging\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            enable_checkpointing=True,\n",
    "            log_every_n_steps=50,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Starting training with PyTorch Lightning...\")\n",
    "        \n",
    "        # Train the model (this replaces all your manual training loop!)\n",
    "        trainer.fit(\n",
    "            model=lightning_model,\n",
    "            train_dataloaders=self.train_loader,\n",
    "            val_dataloaders=self.valid_loader\n",
    "        )\n",
    "        \n",
    "        # Get final metrics\n",
    "        train_metrics = trainer.callback_metrics\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"FINAL TRAINING METRICS:\")\n",
    "        \n",
    "        # Print final metrics\n",
    "        for key, value in train_metrics.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                logger.info(f\"{key}: {value.item():.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Save the trained model\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=lightning_model.model  # Extract the actual model\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\n",
    "        \n",
    "        # Return training history for analysis\n",
    "        return trainer.callback_metrics\n",
    "\n",
    "'''\n",
    "    def train(self):\n",
    "\n",
    "        self.steps_per_epoch = len(self.train_loader)\n",
    "        self.validation_steps = len(self.valid_loader)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        \n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(self.model(inputs), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in self.valid_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    criterion(self.model(inputs), labels) \n",
    "        \n",
    "        logger.info(f\"Training completed for {self.config.params_epochs} epochs\")\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bcbb152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 00:07:48,112: INFO: common]: yaml file successfully load from config/config.yaml\n",
      "[2025-07-06 00:07:48,117: INFO: common]: yaml file successfully load from params.yaml\n",
      "[2025-07-06 00:07:48,118: INFO: common]: directory created at: artifacts\n",
      "[2025-07-06 00:07:48,118: INFO: common]: directory created at: artifacts/model_training\n",
      "[2025-07-06 00:07:48,119: INFO: 1726308249]: Using device: cpu\n",
      "[2025-07-06 00:07:48,143: INFO: 1726308249]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-06 00:07:48,147: INFO: 1726308249]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-06 00:07:48,149: INFO: 1726308249]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-06 00:07:48,149: INFO: 1726308249]: Training samples: 613\n",
      "[2025-07-06 00:07:48,150: INFO: 1726308249]: Validation samples: 72\n",
      "[2025-07-06 00:07:48,150: INFO: 1726308249]: Number of classes: 4\n",
      "[2025-07-06 00:07:48,150: INFO: 1726308249]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-06 00:07:48,176: INFO: setup]: GPU available: True (mps), used: True\n",
      "[2025-07-06 00:07:48,177: INFO: setup]: TPU available: False, using: 0 TPU cores\n",
      "[2025-07-06 00:07:48,177: INFO: setup]: HPU available: False, using: 0 HPUs\n",
      "[2025-07-06 00:07:48,177: INFO: 1726308249]: Starting training with PyTorch Lightning...\n",
      "[2025-07-06 00:07:48,201: INFO: model_summary]: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | Sequential       | 14.8 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "100 K     Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "14.8 M    Total params\n",
      "59.260    Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1xz8vq817d3_x9b35qzvvgjc0000gn/T/ipykernel_79310/1726308249.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 39/39 [00:18<00:00,  2.06it/s, v_num=3]          [2025-07-06 00:08:09,282: INFO: fit_loop]: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|██████████| 39/39 [00:18<00:00,  2.06it/s, v_num=3]\n",
      "[2025-07-06 00:08:49,330: INFO: 1726308249]: Training completed!\n",
      "[2025-07-06 00:08:49,332: INFO: 1726308249]: ============================================================\n",
      "[2025-07-06 00:08:49,332: INFO: 1726308249]: FINAL TRAINING METRICS:\n",
      "[2025-07-06 00:08:49,332: INFO: 1726308249]: train_loss: 2.6374\n",
      "[2025-07-06 00:08:49,333: INFO: 1726308249]: train_acc: 0.6000\n",
      "[2025-07-06 00:08:49,333: INFO: 1726308249]: val_loss: 3.4198\n",
      "[2025-07-06 00:08:49,333: INFO: 1726308249]: val_acc: 0.3472\n",
      "[2025-07-06 00:08:49,333: INFO: 1726308249]: ============================================================\n",
      "[2025-07-06 00:08:49,410: INFO: 1726308249]: Model trained and saved to artifacts/model_training/model.h5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_model_training_config()\n",
    "    training = ModelTraining(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
