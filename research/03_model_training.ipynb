{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4094648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d8c0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvClassifier import logger\n",
    "from cvClassifier.utils.common import get_size, read_yaml, create_directories \n",
    "from cvClassifier.constants import *\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6fb795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data_path: Path\n",
    "    validation_data_path: Path\n",
    "    trained_model_path: Path\n",
    "    best_params_path: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: int\n",
    "    # Hyperparameter search space\n",
    "    learning_rate_range: list  # [min_lr, max_lr]\n",
    "    batch_size_options: list   # [16, 32, 64]\n",
    "    epochs_options: list       # [5, 10, 15]\n",
    "    n_trials: int             # Number of trials for optimization\n",
    "    timeout: int              # Timeout in seconds\n",
    "\n",
    "    mlflow_uri: str\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2aa0ffca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    def get_model_training_config(self) -> ModelTrainingConfig:\\n        ' Gets the config details for the model training pipeline '\\n        config = self.config.model_training\\n        params = self.params\\n        \\n        create_directories([config.root_dir])\\n\\n        model_training_config = ModelTrainingConfig(\\n            root_dir = config.root_dir,\\n            updated_base_model_path = config.updated_base_model_path,\\n            training_data_path = config.training_data,\\n            validation_data_path = config.validation_data,\\n            trained_model_path = config.trained_model_path,\\n            params_epochs = params.EPOCHS,\\n            params_batch_size = params.BATCH_SIZE,\\n            params_is_augmentation = params.AUGMENTATION,\\n            params_image_size = params.IMAGE_SIZE,\\n            params_learning_rate = self.params.LEARNING_RATE\\n        )\\n\\n        return model_training_config\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConfigurationManager:\n",
    "    # this class manages the configuration of the model preparation pipeline\n",
    "\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "            ' Gets the config details for the hyperparameter tuning pipeline '\n",
    "            config = self.config.model_training\n",
    "            params = self.params\n",
    "            training_data = config.training_data\n",
    "            validation_data = config.validation_data\n",
    "\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            model_training_config = ModelTrainingConfig(\n",
    "                root_dir = config.root_dir,\n",
    "                updated_base_model_path = config.updated_base_model_path,\n",
    "                training_data_path = Path(training_data),\n",
    "                validation_data_path = Path(validation_data),\n",
    "                trained_model_path = config.trained_model_path,\n",
    "                best_params_path = Path(config.root_dir) / \"best_params.json\",\n",
    "                params_epochs = params.EPOCHS,\n",
    "                params_batch_size = params.BATCH_SIZE,\n",
    "                params_is_augmentation = params.AUGMENTATION,\n",
    "                params_image_size = params.IMAGE_SIZE,\n",
    "                # Hyperparameter search space\n",
    "                learning_rate_range = params.LEARNING_RATE_RANGE,  # [min_lr, max_lr]\n",
    "                batch_size_options = params.BATCH_SIZE_OPTIONS ,   # Different batch sizes to try\n",
    "                epochs_options = params.EPOCHS_OPTIONS,        # Different epoch counts to try\n",
    "                n_trials = params.N_TRIALS,                       # Number of trials for optimization\n",
    "                timeout = params.TIMEOUT,                        # Timeout in seconds (1 hour)\n",
    "                mlflow_uri = config.mlflow_tracking_uri\n",
    "            )\n",
    "\n",
    "            return model_training_config\n",
    "\n",
    "'''\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        ' Gets the config details for the model training pipeline '\n",
    "        config = self.config.model_training\n",
    "        params = self.params\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            updated_base_model_path = config.updated_base_model_path,\n",
    "            training_data_path = config.training_data,\n",
    "            validation_data_path = config.validation_data,\n",
    "            trained_model_path = config.trained_model_path,\n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_image_size = params.IMAGE_SIZE,\n",
    "            params_learning_rate = self.params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return model_training_config\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff503b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as requests\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b35c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "        \n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate average metrics\n",
    "        if self.test_step_outputs:\n",
    "            avg_loss = torch.stack([x['test_loss'] for x in self.test_step_outputs]).mean()\n",
    "            avg_acc = torch.stack([x['test_acc'] for x in self.test_step_outputs]).mean()\n",
    "            \n",
    "            self.log('avg_test_loss', avg_loss)\n",
    "            self.log('avg_test_acc', avg_acc)\n",
    "            \n",
    "            # Clear the list for next epoch\n",
    "            self.test_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Calculate average metrics\n",
    "        if self.validation_step_outputs:\n",
    "            avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()\n",
    "            avg_acc = torch.stack([x['val_acc'] for x in self.validation_step_outputs]).mean()\n",
    "            \n",
    "            self.log('avg_val_loss', avg_loss)\n",
    "            self.log('avg_val_acc', avg_acc)\n",
    "            \n",
    "            # Clear the list for next epoch\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65a67c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def train(self):\\n        # Create Lightning model\\n        lightning_model = LightningModel(\\n            model=self.model,\\n            learning_rate=self.config.params_learning_rate\\n        )\\n        \\n        # Create trainer with automatic logging and progress bars\\n        trainer = pl.Trainer(\\n            max_epochs=self.config.params_epochs,\\n            accelerator=\\'auto\\',  # Automatically use GPU if available\\n            devices=\\'auto\\',      # Use all available devices\\n            logger=True,         # Enable logging\\n            enable_progress_bar=True,\\n            enable_model_summary=True,\\n            enable_checkpointing=True,\\n            log_every_n_steps=50,\\n        )\\n        \\n        logger.info(\"Starting training with PyTorch Lightning...\")\\n        \\n        # Train the model (this replaces all your manual training loop!)\\n        trainer.fit(\\n            model=lightning_model,\\n            train_dataloaders=self.train_loader,\\n            val_dataloaders=self.valid_loader\\n        )\\n        \\n        # Get final metrics\\n        train_metrics = trainer.callback_metrics\\n        \\n        logger.info(\"Training completed!\")\\n        logger.info(\"=\" * 60)\\n        logger.info(\"FINAL TRAINING METRICS:\")\\n        \\n        # Print final metrics\\n        for key, value in train_metrics.items():\\n            if isinstance(value, torch.Tensor):\\n                logger.info(f\"{key}: {value.item():.4f}\")\\n            else:\\n                logger.info(f\"{key}: {value}\")\\n        \\n        logger.info(\"=\" * 60)\\n        \\n        # Save the trained model\\n        self.save_model(\\n            path=self.config.trained_model_path,\\n            model=lightning_model.model  # Extract the actual model\\n        )\\n        \\n        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\\n        \\n        # Return training history for analysis\\n        return trainer.callback_metrics\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Set up MLflow\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        mlflow.set_experiment(\"hyperparameter_tuning\")\n",
    "\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        logger.info(f\"Model loaded from {self.config.updated_base_model_path}\")\n",
    "\n",
    "    def train_valid_generator(self, batch_size = None):\n",
    "\n",
    "        # preparing the validation dataset\n",
    "        valid_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.config.params_image_size[:-1]),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Converts to tensor and scales to [0,1] (equivalent to rescale=1./255)\n",
    "        ])\n",
    "        \n",
    "        # preparing the training dataset\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.config.params_image_size[:-1]),\n",
    "                transforms.RandomRotation(40),  # rotation_range=40\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # horizontal_flip=True\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.2, 0.2),  # width_shift_range=0.2, height_shift_range=0.2\n",
    "                    scale=(0.8, 1.2),  # zoom_range=0.2\n",
    "                    shear=0.2  # shear_range=0.2\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            train_transforms = valid_transforms\n",
    "\n",
    "\n",
    "        # load training dataset\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=self.config.training_data_path,\n",
    "            transform=train_transforms\n",
    "        )\n",
    "        logger.info(f\"Training dataset created from {self.config.training_data_path}\")\n",
    "\n",
    "        # load validation dataset\n",
    "        valid_dataset = datasets.ImageFolder(\n",
    "            root=self.config.validation_data_path,\n",
    "            transform=valid_transforms\n",
    "        )\n",
    "        logger.info(f\"Validation dataset created from {self.config.validation_data_path}\")\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.params_batch_size if batch_size == None else batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.config.params_batch_size if batch_size == None else batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.train_dataset_size = len(train_dataset)\n",
    "        self.valid_dataset_size = len(valid_dataset)\n",
    "        \n",
    "        logger.info(f\"Training samples: {self.train_dataset_size}\")\n",
    "        logger.info(f\"Validation samples: {self.valid_dataset_size}\")\n",
    "        logger.info(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "        logger.info(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "        return self.train_loader, self.valid_loader\n",
    "    \n",
    "    def get_hyperparameter_search_space(self):\n",
    "        \"\"\"Define hyperparameter search space\"\"\"\n",
    "        return {\n",
    "            'learning_rate': self.config.learning_rate_range,\n",
    "            'batch_size': self.config.batch_size_options,\n",
    "            'epochs': self.config.epochs_options\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    \n",
    "        search_space = self.get_hyperparameter_search_space()\n",
    "    \n",
    "        # Suggest hyperparameters using the search space\n",
    "        learning_rate = trial.suggest_float('learning_rate', \n",
    "                                        search_space['learning_rate'][0], \n",
    "                                        search_space['learning_rate'][-1], \n",
    "                                        log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', search_space['batch_size'])\n",
    "        epochs = trial.suggest_categorical('epochs', search_space['epochs'])\n",
    "        \n",
    "        logger.info(f\"Trial {trial.number}: lr={learning_rate:.6f}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "        with mlflow.start_run(nested=True) as run:\n",
    "            \n",
    "            # log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                'learning_rate': learning_rate,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs\n",
    "            })\n",
    "            # get base model\n",
    "            self.get_base_model()\n",
    "            # Create data loaders with suggested batch size\n",
    "            train_loader, valid_loader = self.train_valid_generator(batch_size)\n",
    "\n",
    "            # create lightning model instance\n",
    "            lightning_model = LightningModel(\n",
    "                model=self.model,\n",
    "                learning_rate=learning_rate\n",
    "            )\n",
    "\n",
    "            # create an mlflow logger\n",
    "            mlflow_logger = MLFlowLogger(\n",
    "                experiment_name=\"hyperparameter_tuning\",\n",
    "                tracking_uri=self.config.mlflow_uri,\n",
    "                run_id=run.info.run_id # Associated with the current MLflow run\n",
    "            )\n",
    "\n",
    "            # create new pytorch lightning trainer\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=epochs,\n",
    "                accelerator='auto',\n",
    "                devices='auto',\n",
    "                logger=mlflow_logger,\n",
    "                enable_progress_bar=False, # Disable progress bar for cleaner Optuna output\n",
    "                enable_model_summary=False,\n",
    "                log_every_n_steps=10\n",
    "            )\n",
    "\n",
    "            # fit the model\n",
    "            trainer.fit(\n",
    "                model=lightning_model,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=valid_loader\n",
    "            )\n",
    "\n",
    "            # Retrieve metrics from trainer.callback_metrics after training\n",
    "            val_loss = trainer.callback_metrics.get('val_loss')\n",
    "            val_acc = trainer.callback_metrics.get('val_acc')\n",
    "            train_loss = trainer.callback_metrics.get('train_loss')\n",
    "            train_acc = trainer.callback_metrics.get('train_acc')\n",
    "\n",
    "            # log metrics to mlflow\n",
    "            trial_metrics = {}\n",
    "            if val_loss is not None:\n",
    "                trial_metrics['val_loss'] = val_loss.item() if hasattr(val_loss, 'item') else val_loss\n",
    "            if val_acc is not None:\n",
    "                trial_metrics['val_acc'] = val_acc.item() if hasattr(val_acc, 'item') else val_acc\n",
    "            if train_loss is not None:\n",
    "                trial_metrics['train_loss'] = train_loss.item() if hasattr(train_loss, 'item') else train_loss\n",
    "            if train_acc is not None:\n",
    "                trial_metrics['train_acc'] = train_acc.item() if hasattr(train_acc, 'item') else train_acc\n",
    "\n",
    "            mlflow.log_metrics(trial_metrics)\n",
    "\n",
    "            # Return the validation loss for Optuna to minimize\n",
    "            if 'val_loss' in trial_metrics:\n",
    "                return trial_metrics['val_loss']\n",
    "            else:\n",
    "                logger.warning(f\"Validation loss not found for Trial {trial.number}. Returning inf.\")\n",
    "                return float('inf')\n",
    "    \n",
    "\n",
    "    def optimize_hyperparameters(self):\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=self.config.n_trials,\n",
    "            timeout=self.config.timeout\n",
    "        )\n",
    "        \n",
    "        # Get best parameters\n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        logger.info(f\"Best parameters: {self.best_params}\")\n",
    "        logger.info(f\"Best validation loss: {self.best_score:.4f}\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        best_params_path = Path(self.config.root_dir)/\"best_params.json\"\n",
    "        with open(best_params_path, 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=4)\n",
    "        \n",
    "        return self.best_params\n",
    "\n",
    "    def log_trial_to_mlflow(self, trial_params, trial_metrics, lightning_model):\n",
    "        \"\"\"Log trial results to MLflow\"\"\"\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.log_params(trial_params)\n",
    "            mlflow.log_metrics(trial_metrics)\n",
    "            \n",
    "            # Log model\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.pytorch.log_model(\n",
    "                    lightning_model.model, \n",
    "                    \"model\", \n",
    "                    registered_model_name=\"VGG16Model\"\n",
    "                )\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(lightning_model.model, \"model\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train with hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Run hyperparameter optimization first\n",
    "        self.optimize_hyperparameters()\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        return self.train_with_best_params()\n",
    "\n",
    "    def train_with_best_params(self):\n",
    "        \"\"\"Train the final model with best hyperparameters\"\"\"\n",
    "        if self.best_params is None:\n",
    "            logger.warning(\"No best parameters found. Running optimization first...\")\n",
    "            self.optimize_hyperparameters()\n",
    "        \n",
    "        # Create data loaders with best batch size\n",
    "        train_loader, valid_loader = self.train_valid_generator(self.best_params['batch_size'])\n",
    "        \n",
    "        # Create Lightning model with best learning rate\n",
    "        lightning_model = LightningModel(\n",
    "            model=self.model,\n",
    "            learning_rate=self.best_params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Set up model checkpointing\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=self.config.root_dir,\n",
    "            filename=\"best_model\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1\n",
    "        )\n",
    "\n",
    "        # Set up MLflow logger for final training\n",
    "        mlflow_logger = MLFlowLogger(\n",
    "            experiment_name=\"best_parameters_training\",\n",
    "            tracking_uri=self.config.mlflow_uri\n",
    "        )\n",
    "        \n",
    "        # Create trainer with best epochs\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.best_params['epochs'],\n",
    "            accelerator='auto',\n",
    "            devices='auto',\n",
    "            logger=mlflow_logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            log_every_n_steps=50,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.fit(\n",
    "            model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader\n",
    "        )\n",
    "        \n",
    "        # Save the final model\n",
    "        torch.save(lightning_model.model, self.config.trained_model_path)\n",
    "        \n",
    "        logger.info(f\"Final model saved to {self.config.trained_model_path}\")\n",
    "        \n",
    "        return trainer.callback_metrics\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    def train(self):\n",
    "        # Create Lightning model\n",
    "        lightning_model = LightningModel(\n",
    "            model=self.model,\n",
    "            learning_rate=self.config.params_learning_rate\n",
    "        )\n",
    "        \n",
    "        # Create trainer with automatic logging and progress bars\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.config.params_epochs,\n",
    "            accelerator='auto',  # Automatically use GPU if available\n",
    "            devices='auto',      # Use all available devices\n",
    "            logger=True,         # Enable logging\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            enable_checkpointing=True,\n",
    "            log_every_n_steps=50,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Starting training with PyTorch Lightning...\")\n",
    "        \n",
    "        # Train the model (this replaces all your manual training loop!)\n",
    "        trainer.fit(\n",
    "            model=lightning_model,\n",
    "            train_dataloaders=self.train_loader,\n",
    "            val_dataloaders=self.valid_loader\n",
    "        )\n",
    "        \n",
    "        # Get final metrics\n",
    "        train_metrics = trainer.callback_metrics\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"FINAL TRAINING METRICS:\")\n",
    "        \n",
    "        # Print final metrics\n",
    "        for key, value in train_metrics.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                logger.info(f\"{key}: {value.item():.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Save the trained model\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=lightning_model.model  # Extract the actual model\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model trained and saved to {self.config.trained_model_path}\")\n",
    "        \n",
    "        # Return training history for analysis\n",
    "        return trainer.callback_metrics\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbb152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-13 23:39:01,936: INFO: common]: yaml file successfully loaded from config/config.yaml\n",
      "[2025-07-13 23:39:01,940: INFO: common]: yaml file successfully loaded from params.yaml\n",
      "[2025-07-13 23:39:01,941: INFO: common]: Directory created at: artifacts\n",
      "[2025-07-13 23:39:01,942: INFO: common]: Directory created at: artifacts/model_training\n",
      "[2025-07-13 23:39:02,102: INFO: 1728642959]: Using device: cpu\n",
      "[2025-07-13 23:39:02,152: INFO: 1728642959]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-13 23:39:02,156: INFO: 1728642959]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-13 23:39:02,157: INFO: 1728642959]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-13 23:39:02,158: INFO: 1728642959]: Training samples: 613\n",
      "[2025-07-13 23:39:02,158: INFO: 1728642959]: Validation samples: 72\n",
      "[2025-07-13 23:39:02,159: INFO: 1728642959]: Number of classes: 4\n",
      "[2025-07-13 23:39:02,159: INFO: 1728642959]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-13 23:39:02,159: INFO: 1728642959]: Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1xz8vq817d3_x9b35qzvvgjc0000gn/T/ipykernel_53816/1728642959.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
      "[I 2025-07-13 23:39:02,160] A new study created in memory with name: no-name-ec8e2b26-3dd6-4026-825d-afa78cd53fe0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-13 23:39:02,161: INFO: 1728642959]: Trial 0: lr=0.005612, batch_size=8, epochs=50\n",
      "[2025-07-13 23:39:02,559: INFO: 1728642959]: Model loaded from artifacts/model_preparation/updated_base_model.pth\n",
      "[2025-07-13 23:39:02,562: INFO: 1728642959]: Training dataset created from artifacts/data_ingestion/Data/train\n",
      "[2025-07-13 23:39:02,563: INFO: 1728642959]: Validation dataset created from artifacts/data_ingestion/Data/valid\n",
      "[2025-07-13 23:39:02,563: INFO: 1728642959]: Training samples: 613\n",
      "[2025-07-13 23:39:02,563: INFO: 1728642959]: Validation samples: 72\n",
      "[2025-07-13 23:39:02,564: INFO: 1728642959]: Number of classes: 4\n",
      "[2025-07-13 23:39:02,564: INFO: 1728642959]: Classes: ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n",
      "[2025-07-13 23:39:02,595: INFO: setup]: GPU available: True (mps), used: True\n",
      "[2025-07-13 23:39:02,595: INFO: setup]: TPU available: False, using: 0 TPU cores\n",
      "[2025-07-13 23:39:02,595: INFO: setup]: HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/cv-cancer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_model_training_config()\n",
    "    training = ModelTraining(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
